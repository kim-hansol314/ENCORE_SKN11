{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사하강법 (Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잔차제곱합 (Residual Sum of Squares, RSS)\n",
    "- 잔차 = 실제 값 - 예측 값\n",
    "- 잔차제곱합 = (실제 값 - 예측 값)의 제곱의 합\n",
    "- 회귀 모델의 정확도를 측정하는 지표\n",
    "    - RSS가 작을수록 정확하게 예측하는 모델\n",
    "    - RSS가 클수록 잘못된 예측하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 회귀 모델은 RSS가 최소가 되는 방향으로 학습이 진행됨 = 회귀계수(절편)는 RSS가 최소가 되도록 학습\n",
    "- 비용함수 R(w)가 가장 작을 때의 w를 찾는 것이 회귀 모델의 목표\n",
    "    - 매 회차에 계산된 R(w)에서 순간변화율(기울기)를 구해야 함 -> 미분 사용\n",
    "    - 단, 우리가 구해야 하는 회귀계수는 하나 이상이므로 우리는 편미분을 사용함\n",
    "        - w0(절편)을 고정한 채로 w1의 미분을 구하고, w1을 고정한 채로 w0 미분을 구함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습률 (Learning Rate)\n",
    "- 최적의 해를 빠르게 혹은 천천히 조금씩 찾아가는 '정도'를 가르키는 하이퍼 파라미터\n",
    "- 기본 값으로 보통 0.001을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**경사하강법 수식**\n",
    "\n",
    "$w_1$ $w_0$을 반복적으로 업데이트하며 최적의 회귀계수를 찾음\n",
    "<br/>\n",
    "$w_1 = w_1 - (-η\\frac{2}{N}\\sum^{N}_{i=1} x_i * (실제값_i - 예측값_i))$\n",
    "<br/>\n",
    "$w_0 = w_0 - (-η\\frac{2}{N}\\sum^{N}_{i=1}(실제값_i - 예측값_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**경사하강법 공식**\n",
    "\n",
    "$w1 = w1 - (미분값)$\n",
    "\n",
    "$w1 = w1 - (-학습률 * 2 / N * (x * (실제값 - 예측값))의 합)$\n",
    "\n",
    "$w0 = w0 - (미분값)$\n",
    "\n",
    "$w0 = w0 - (-학습률 * 2 / N * (실제값 - 예측값)의 합)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# 0~1 사이의 100개의 난수 생성\n",
    "X = np.random.rand(100, 1)\n",
    "# print(X.shape)\n",
    "# print(X)\n",
    "\n",
    "noise = np.random.randn(100, 1)  # 정규분포 난수를 노이즈로 사용\n",
    "y = 6 + 4 * X + noise\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법 실행\n",
    "\n",
    "# 회귀계수(가중치) 초기화\n",
    "w0 = np.zeros((1, 1))\n",
    "w1 = np.zeros((1, 1))\n",
    "\n",
    "# 잔차 계산\n",
    "y_pred = w0 + np.dot(X, w1)\n",
    "diff = y - y_pred\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 데이터 개수\n",
    "N = len(X)\n",
    "\n",
    "\n",
    "# w0 편미분 (w0를 갱신할 값)\n",
    "# w0 = w0 - (-학습률 * 2 / N * (실제값 - 예측값)의 합)\n",
    "w0_diff = -learning_rate * 2 / N * np.sum(diff)\n",
    "# 절편(w0) 갱신\n",
    "w0 = w0 - w0_diff\n",
    "\n",
    "# w1 편미분 (w1를 갱신할 값)\n",
    "# w1 = w1 - (-학습률 * 2 / N * (x * (실제값 - 예측값))의 합)\n",
    "w1_diff = -learning_rate * 2 / N * np.dot(X.T, diff) \n",
    "# 가중치(w1) 갱신\n",
    "w1 = w1 - w1_diff\n",
    "\n",
    "print(f'1회 업데이트된 회귀계수 w0: {w0}, w1: {w1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, learning_rate=0.01, max_iter=1000):\n",
    "\n",
    "    # 회귀계수(가중치) 초기화\n",
    "    w0 = np.zeros((1, 1))\n",
    "    w1 = np.zeros((1, 1))\n",
    "\n",
    "    # 학습률\n",
    "    # learning_rate = 0.01\n",
    "\n",
    "    # 데이터 개수\n",
    "    N = len(X)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "\n",
    "        # 잔차 계산\n",
    "        y_pred = w0 + np.dot(X, w1)\n",
    "        diff = y - y_pred\n",
    "\n",
    "        # w0 편미분 (w0를 갱신할 값)\n",
    "        # w0 = w0 - (-학습률 * 2 / N * (실제값 - 예측값)의 합)\n",
    "        w0_diff = -learning_rate * 2 / N * np.sum(diff)\n",
    "        # 절편(w0) 갱신\n",
    "        w0 = w0 - w0_diff\n",
    "\n",
    "        # w1 편미분 (w1를 갱신할 값)\n",
    "        # w1 = w1 - (-학습률 * 2 / N * (x * (실제값 - 예측값))의 합)\n",
    "        w1_diff = -learning_rate * 2 / N * np.dot(X.T, diff) \n",
    "        # 가중치(w1) 갱신\n",
    "        w1 = w1 - w1_diff\n",
    "\n",
    "        # 시각화\n",
    "        plt.figure(figsize=(3, 2))\n",
    "        plt.scatter(X, y)\n",
    "        plt.plot(X, y_pred, color='red')\n",
    "        plt.show()\n",
    "\n",
    "    return w0, w1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred) ** 2) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, w1 = gradient_descent(X, y, max_iter=300)\n",
    "\n",
    "print('최종 회귀 계수(가중치)', w1)\n",
    "print('최종 회귀 계수(절편)', w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w0 + np.dot(X, w1)\n",
    "print('비용 함수 결과:', cost_function(y, y_pred))\n",
    "\n",
    "# 비용 함수 결과: 1.4569510352266337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 미니 배치 경사하강법 (Mini-batch Gradient Descent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X, y, batch_size=10,learning_rate=0.01, max_iter=1000):\n",
    "\n",
    "    # 회귀계수(가중치) 초기화\n",
    "    w0 = np.zeros((1, 1))\n",
    "    w1 = np.zeros((1, 1))\n",
    "\n",
    "    # 학습률\n",
    "    # learning_rate = 0.01\n",
    "\n",
    "    # 데이터 개수\n",
    "    N = batch_size\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # 미니배치 데이터 선정\n",
    "        random_index = np.random.permutation(X.shape[0])\n",
    "        X_sample = X[random_index[:batch_size]]\n",
    "        y_sample = y[random_index[:batch_size]]\n",
    "\n",
    "        # 잔차 계산\n",
    "        y_pred = w0 + np.dot(X_sample, w1)\n",
    "        diff = y_sample - y_pred\n",
    "\n",
    "        # w0 편미분 (w0를 갱신할 값)\n",
    "        # w0 = w0 - (-학습률 * 2 / N * (실제값 - 예측값)의 합)\n",
    "        w0_diff = -learning_rate * 2 / N * np.sum(diff)\n",
    "        # 절편(w0) 갱신\n",
    "        w0 = w0 - w0_diff\n",
    "\n",
    "        # w1 편미분 (w1를 갱신할 값)\n",
    "        # w1 = w1 - (-학습률 * 2 / N * (x * (실제값 - 예측값))의 합)\n",
    "        w1_diff = -learning_rate * 2 / N * np.dot(X_sample.T, diff) \n",
    "        # 가중치(w1) 갱신\n",
    "        w1 = w1 - w1_diff\n",
    "\n",
    "        # 시각화\n",
    "        plt.figure(figsize=(3, 2))\n",
    "        plt.scatter(X, y)\n",
    "        plt.plot(X_sample, y_pred, color='red')\n",
    "        plt.show()\n",
    "\n",
    "    return w0, w1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, w1 = mini_batch_gradient_descent(X, y, max_iter=300)\n",
    "\n",
    "print('최종 회귀 계수(가중치)', w1) # model.coef_\n",
    "print('최종 회귀 계수(절편)', w0)   # model.intercept_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
