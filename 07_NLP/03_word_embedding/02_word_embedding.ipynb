{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4d99b8",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba8fda",
   "metadata": {},
   "source": [
    "- **Word Embedding**은 단어를 고정된 차원의 벡터로 변환하는 기술로, \n",
    "단어 간의 의미적 유사성을 반영하도록 학습된 벡터를 말한다.\n",
    "- 이 기술은 자연어 처리에서 문장을 처리하고 이해하는 데 활용된다.\n",
    "- 숫자로 표현되 단어 목록을 통해 감정을 추출하는 것도 가능하다.\n",
    "- 연관성 있는 단어들을 군집화하여 다차원 공간에 벡터로 나타낼 수 있으며,\n",
    "이는 단어나 문장을 벡터 공간에 매핑하는 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c326fb",
   "metadata": {},
   "source": [
    "**Embedding Matrix 예시**\n",
    "\n",
    "*아래 표의 벡터 값들은 모두 기계 학습을 통해 학습된 결과이다.*  \n",
    "\n",
    "| Dimension | Man (5391) | Woman (9853) | King (4914) | Queen (7157) | Apple (456) | Orange (6257) |\n",
    "|-----------|------------|--------------|-------------|--------------|-------------|---------------|\n",
    "| 성별      | -1         | 1            | -0.95       | 0.97         | 0.00        | 0.01          |\n",
    "| 귀족      | 0.01       | 0.02         | 0.93        | 0.95         | -0.01       | 0.00          |\n",
    "| 나이      | 0.03       | 0.02         | 0.7         | 0.69         | 0.03        | -0.02         |\n",
    "| 음식      | 0.04       | 0.01         | 0.02        | 0.01         | 0.95        | 0.97          |\n",
    "\n",
    "<br>\n",
    "\n",
    "*아래는 전치된 표이다.*\n",
    "\n",
    "| Word          | 성별   | 귀족   | 나이   | 음식   |\n",
    "|---------------|--------|--------|--------|--------|\n",
    "| Man (5391)    | -1.00  | 0.01   | 0.03   | 0.04   |\n",
    "| Woman (9853)  | 1.00   | 0.02   | 0.02   | 0.01   |\n",
    "| King (4914)   | -0.95  | 0.93   | 0.70   | 0.02   |\n",
    "| Queen (7157)  | 0.97   | 0.95   | 0.69   | 0.01   |\n",
    "| Apple (456)   | 0.00   | -0.01  | 0.03   | 0.95   |\n",
    "| Orange (6257) | 0.01   | 0.00   | -0.02  | 0.97   |\n",
    "\n",
    "- **의미적 유사성 반영**  \n",
    "  - 단어를 고정된 크기의 실수 벡터로 표현하며, 비슷한 의미를 가진 단어는 벡터 공간에서 가깝게 위치한다.  \n",
    "  - 예를 들어, \"king\"과 \"queen\"은 비슷한 맥락에서 자주 사용되므로 벡터 공간에서 가까운 위치에 배치된다.  \n",
    "\n",
    "- **밀집 벡터(Dense Vector)**  \n",
    "  - BoW, DTM, TF-IDF와 달리 Word Embedding은 저차원 밀집 벡터로 변환되며, 차원이 낮으면서도 의미적으로 풍부한 정보를 담는다.  \n",
    "  - 벡터 차원은 보통 100 또는 300 정도로 제한된다.  \n",
    "\n",
    "- **문맥 정보 반영**  \n",
    "  - Word Embedding은 단어 주변의 단어들을 학습해 단어의 의미를 추론한다.  \n",
    "  - 예를 들어, \"bank\"라는 단어가 \"river\"와 함께 나오면 \"강둑\"을, \"money\"와 함께 나오면 \"은행\"을 의미한다고 학습한다.  \n",
    "\n",
    "- **학습 기반 벡터**  \n",
    "  - Word Embedding은 대규모 텍스트 데이터에서 단어 간 연관성을 학습해 벡터를 생성한다.  \n",
    "  - 반면, BoW나 TF-IDF는 단순한 규칙 기반 벡터화 방법이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4583e7",
   "metadata": {},
   "source": [
    "### 희소 표현(Sparse Representation) | 분산 표현(Distributed Representation)\n",
    "- 원-핫 인코딩으로 얻은 원-핫 벡터는 단어의 인덱스 값만 1이고 나머지는 모두 0으로 표현된다.\n",
    "- 이렇게 대부분의 값이 0인 벡터나 행렬을 사용하는 표현 방식을 희소 표현(sparse representation)이라고 한다.  \n",
    "- 희소 표현은 단어 벡터 간 유의미한 유사성을 표현할 수 없다는 단점이 있다.\n",
    "- 이를 해결하기 위해 단어의 의미를 다차원 공간에 벡터화하는 분산 표현(distributed representation)을 사용한다.\n",
    "- 분산 표현으로 단어 간 의미적 유사성을 벡터화하는 작업을 워드 임베딩(embedding)이라고 하며, 이렇게 변환된 벡터를 임베딩 벡터(embedding vector)라고 한다.  \n",
    "- **원-핫 인코딩 → 희소 표현**  \n",
    "- **워드 임베딩 → 분산 표현**  \n",
    "\n",
    "**분산 표현(Distributed Representation)**\n",
    "- 분산 표현은 분포 가설(distributional hypothesis)에 기반한 방법이다.\n",
    "- 이 가설은 \"비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다\"는 내용을 전제로 한다.\n",
    "- 예를 들어, '강아지'라는 단어는 '귀엽다', '예쁘다', '애교' 등의 단어와 함께 자주 등장하며, 이를 벡터화하면 해당 단어들은 유사한 벡터값을 갖게 된다.\n",
    "- 분산 표현은 단어의 의미를 여러 차원에 걸쳐 분산하여 표현한다.  \n",
    "- 이 방식은 원-핫 벡터처럼 단어 집합 크기만큼의 차원이 필요하지 않으며, 상대적으로 저차원으로 줄어든다.\n",
    "- 예를 들어, 단어 집합 크기가 10,000이고 '강아지'의 인덱스가 4라면, 원-핫 벡터는 다음과 같다:\n",
    "  \n",
    "- **강아지 = [0 0 0 0 1 0 0 ... 0]** (뒤에 9,995개의 0 포함)  \n",
    "- 그러나 Word2Vec으로 임베딩된 벡터는 단어 집합 크기와 무관하며, 설정된 차원의 수만큼 실수값을 가진 벡터가 된다:  \n",
    "- **강아지 = [0.2 0.3 0.5 0.7 0.2 ... 0.2]**  \n",
    "\n",
    "**요약하면,**\n",
    "- 희소 표현은 고차원에서 각 차원이 분리된 방식으로 단어를 표현하지만, 분산 표현은 저차원에서 단어의 의미를 여러 차원에 분산시켜 표현한다.\n",
    "- 이를 통해 단어 벡터 간 유의미한 유사도를 계산할 수 있으며, 대표적인 학습 방법으로 Word2Vec이 사용된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257fec89",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- 2013년 구글에서 개발한 Word Embedding 방법\n",
    "- 최초의 neural embedding model\n",
    "- 매우 큰 corpus에서 자동 학습\n",
    "    - 비지도 지도 학습 (자기 지도학습)이라 할 수 있음\n",
    "    - 많은 데이터를 기반으로 label 값 유추하고 이를 지도학습에 사용\n",
    "- ex) \n",
    "    - **이사금**께 충성을 맹세하였다.\n",
    "    - **왕**께 충성을 맹세하였다.\n",
    "\n",
    "**WordVec 훈련방식에 따른 구분**\n",
    "1. CBOW : 주변 단어로 중심 단어를 예측\n",
    "2. Skip-gram : 중심 단어로 주변 단어를 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e86e21",
   "metadata": {},
   "source": [
    "##### CBOW (Continuous Bag of Words)  \n",
    "- CBOW는 원-핫 벡터를 사용하지만, 이는 단순히 위치를 가리킬 뿐 vocabulary를 직접적으로 참조하지 않는다.  \n",
    "\n",
    "**예시:**  \n",
    "\n",
    "> The fat cat sat on the mat  \n",
    "\n",
    "주어진 문장에서 'sat'이라는 단어를 예측하는 것이 CBOW의 주요 작업이다.  \n",
    "- **중심 단어(center word):** 예측하려는 단어 ('sat')  \n",
    "- **주변 단어(context word):** 예측에 사용되는 단어들  \n",
    "\n",
    "중심 단어를 예측하기 위해 앞뒤 몇 개의 단어를 참고할지 결정하는 범위를 **윈도우(window)**라고 한다.  \n",
    "예를 들어, 윈도우 크기가 2이고 중심 단어가 'sat'라면, 앞의 두 단어(fat, cat)와 뒤의 두 단어(on, the)를 입력으로 사용한다.  \n",
    "윈도우 크기가 n일 경우, 참고하는 주변 단어의 개수는 총 2n이다. 윈도우를 옆으로 이동하며 학습 데이터를 생성하는 방법을 **슬라이딩 윈도우(sliding window)**라고 한다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG)\n",
    "\n",
    "\n",
    "**훈련 과정**\n",
    "\n",
    "CBOW는 embedding 벡터를 학습하기 위한 구조를 갖는다. 초기에는 가중치가 임의의 값으로 설정되며, 역전파를 통해 최적화된다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG)\n",
    "\n",
    "Word2Vec은 은닉층이 하나뿐인 얕은 신경망(shallow neural network) 구조를 사용한다.  \n",
    "학습 대상이 되는 주요 가중치는 두 가지이다:  \n",
    "\n",
    "1. **투사층(projection layer):**  \n",
    "   - 활성화 함수가 없으며 룩업 테이블 연산을 담당한다.  \n",
    "   - 입력층과 투사층 사이의 가중치 W는 V × M 행렬로 표현되며, 여기서 **V는 단어 집합의 크기, M은 벡터의 차원**이다.  \n",
    "   - W 행렬의 각 행은 학습 후 단어의 M차원 임베딩 벡터로 간주된다.  \n",
    "   - 예를 들어, 벡터 차원을 5로 설정하면 각 단어의 임베딩 벡터는 5차원이 된다.  \n",
    "\n",
    "2. **출력층:**  \n",
    "   - 투사층과 출력층 사이의 가중치 W'는 M × V 행렬로 표현된다.  \n",
    "   - 이 두 행렬(W와 W')은 서로 독립적이며, 학습 전에는 랜덤 값으로 초기화된다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_3.PNG)\n",
    "\n",
    "\n",
    "**예측 과정**\n",
    "1. CBOW는 계산된 룩업 테이블의 평균을 구한 뒤, 출력층의 가중치 W'와 내적한다.  \n",
    "2. 결과값은 **소프트맥스(softmax)** 활성화 함수에 입력되어, 중심 단어일 확률을 나타내는 예측값으로 변환된다.  \n",
    "3. 출력된 예측값(스코어 벡터)은 실제 타겟 원-핫 벡터와 비교되며, **크로스 엔트로피(cross-entropy)** 함수로 손실값을 계산한다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_5.PNG)\n",
    "\n",
    "손실 함수 식:  \n",
    "$\n",
    "cost(\\hat{y}, y) = -\\sum_{j=1}^{V} y_{j} \\cdot log(\\hat{y}_{j})\n",
    "$  \n",
    "\n",
    "여기서, $\\hat{y}_{j}$는 예측 확률, $y_{j}$는 실제 값이며, V는 단어 집합의 크기를 의미한다.  \n",
    "\n",
    "\n",
    "**학습 결과**  \n",
    "- 역전파를 통해 가중치 W와 W'가 학습된다. \n",
    "- 학습이 완료되면 W 행렬의 각 행을 단어의 임베딩 벡터로 사용하거나, W와 W' 모두를 이용해 임베딩 벡터를 생성할 수 있다.  \n",
    "- CBOW는 주변 단어를 기반으로 중심 단어를 예측하는 구조를 갖추고 있으며, 이를 통해 단어 간 의미적 관계를 효과적으로 학습할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf5f82a",
   "metadata": {},
   "source": [
    "##### Skip-gram\n",
    "- Skip-gram은 중심 단어에서 주변 단어를 예측한다.\n",
    "- 윈도우 크기가 2일 때, 데이터셋은 다음과 같이 구성된다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/skipgram_dataset.PNG)\n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG)\n",
    "\n",
    "- 중심 단어에 대해서 주변 단어를 예측하므로 투사층에서 벡터들의 평균을 구하는 과정은 없다.\n",
    "- 여러 논문에서 성능 비교를 진행했을 때 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074ebf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\maest\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\maest\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\maest\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\maest\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\maest\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a763fb",
   "metadata": {},
   "source": [
    "### 영어 Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd06071",
   "metadata": {},
   "source": [
    "- 데이터 취득 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99098a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=1TF1yAHF3qRINbXWFOajFjUCxUF64QZMX\"\n",
    "# output = \"ted_en.xml\"\n",
    "\n",
    "# gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f06443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  lxml import etree\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5422748d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24222849\n",
      "24062319\n"
     ]
    }
   ],
   "source": [
    "#xml 데이터 처리\n",
    "f = open('ted_en.xml', 'r', encoding='UTF-8')\n",
    "xml = etree.parse(f)\n",
    "\n",
    "contents = xml.xpath('//content/text()')\n",
    "# contents[:5]\n",
    "\n",
    "corpus = '\\n'.join(contents)   #각각 들어있는 것들을 corpus로 만들어줌.\n",
    "print(len(corpus))\n",
    "\n",
    "#정규식을 이용해 (Laughter), (Applause)등 키워드 제거\n",
    "corpus = re.sub(r'\\([^)]*\\)', '', corpus)\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a90f714a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['two', 'reasons', 'companies', 'fail', 'new'],\n",
       " ['real',\n",
       "  'real',\n",
       "  'solution',\n",
       "  'quality',\n",
       "  'growth',\n",
       "  'figuring',\n",
       "  'balance',\n",
       "  'two',\n",
       "  'activities',\n",
       "  'exploration',\n",
       "  'exploitation'],\n",
       " ['necessary', 'much', 'good', 'thing'],\n",
       " ['consider', 'facit'],\n",
       " ['actually', 'old', 'enough', 'remember']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 전처리 (토큰화, 대소문자 정규화, 불용어 처리)\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "preprocessed_sentences = []\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence =  sentence.lower()\n",
    "    sentence = re.sub(r'[^a-z0-9]', ' ', sentence) #영소문자, 숫자 외 제거\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [token for token in tokens if token not in en_stopwords]\n",
    "    preprocessed_sentences.append(tokens)\n",
    "\n",
    "preprocessed_sentences[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c25462",
   "metadata": {},
   "source": [
    "### Embedding 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79bc72cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21462, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Embedding 모델 학습\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences = preprocessed_sentences, #corpus\n",
    "    vector_size = 100,                  #임베딩 벡터 차원\n",
    "    sg=0,                               #학습 알고리즘 선택(0=CROW, 1=Skip-gram)\n",
    "    window = 5,                         #주변 단어 수 (앞뒤로 n개 고려)\n",
    "    min_count = 5                       #최소 빈도 (빈도  n개미만은 제거)\n",
    ")\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8ee1f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>-0.835178</td>\n",
       "      <td>-0.119281</td>\n",
       "      <td>-0.440201</td>\n",
       "      <td>0.192750</td>\n",
       "      <td>0.073644</td>\n",
       "      <td>-0.699794</td>\n",
       "      <td>0.977574</td>\n",
       "      <td>0.578437</td>\n",
       "      <td>-2.640824</td>\n",
       "      <td>-0.215296</td>\n",
       "      <td>...</td>\n",
       "      <td>2.062862</td>\n",
       "      <td>0.336131</td>\n",
       "      <td>0.076664</td>\n",
       "      <td>0.108958</td>\n",
       "      <td>0.304847</td>\n",
       "      <td>-1.532940</td>\n",
       "      <td>-0.666886</td>\n",
       "      <td>-0.108505</td>\n",
       "      <td>0.761793</td>\n",
       "      <td>0.945292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>-1.898524</td>\n",
       "      <td>0.805438</td>\n",
       "      <td>0.192769</td>\n",
       "      <td>0.112155</td>\n",
       "      <td>0.050628</td>\n",
       "      <td>-1.142790</td>\n",
       "      <td>-0.281292</td>\n",
       "      <td>1.008322</td>\n",
       "      <td>-1.641191</td>\n",
       "      <td>-2.923650</td>\n",
       "      <td>...</td>\n",
       "      <td>1.139784</td>\n",
       "      <td>0.489528</td>\n",
       "      <td>-1.488058</td>\n",
       "      <td>-1.124601</td>\n",
       "      <td>-0.585902</td>\n",
       "      <td>0.197538</td>\n",
       "      <td>-1.239247</td>\n",
       "      <td>-0.041986</td>\n",
       "      <td>-1.946663</td>\n",
       "      <td>1.459767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>-0.852569</td>\n",
       "      <td>0.073068</td>\n",
       "      <td>-0.795189</td>\n",
       "      <td>-0.852964</td>\n",
       "      <td>0.465843</td>\n",
       "      <td>-0.275568</td>\n",
       "      <td>0.171036</td>\n",
       "      <td>1.509210</td>\n",
       "      <td>-0.715329</td>\n",
       "      <td>0.387317</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233583</td>\n",
       "      <td>0.759793</td>\n",
       "      <td>0.243843</td>\n",
       "      <td>-0.210720</td>\n",
       "      <td>0.659490</td>\n",
       "      <td>0.670538</td>\n",
       "      <td>0.884029</td>\n",
       "      <td>0.539952</td>\n",
       "      <td>0.519874</td>\n",
       "      <td>-0.314470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>-0.810799</td>\n",
       "      <td>0.276093</td>\n",
       "      <td>-0.233160</td>\n",
       "      <td>-0.071715</td>\n",
       "      <td>-0.062151</td>\n",
       "      <td>0.136346</td>\n",
       "      <td>-0.144619</td>\n",
       "      <td>-0.011745</td>\n",
       "      <td>-0.355997</td>\n",
       "      <td>-1.143154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705935</td>\n",
       "      <td>-0.117349</td>\n",
       "      <td>-0.209008</td>\n",
       "      <td>-0.275618</td>\n",
       "      <td>-0.320133</td>\n",
       "      <td>0.630917</td>\n",
       "      <td>0.416345</td>\n",
       "      <td>-0.668252</td>\n",
       "      <td>-0.038271</td>\n",
       "      <td>-0.100392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>going</th>\n",
       "      <td>-1.182257</td>\n",
       "      <td>1.100788</td>\n",
       "      <td>-0.105712</td>\n",
       "      <td>-0.579604</td>\n",
       "      <td>0.413323</td>\n",
       "      <td>0.278536</td>\n",
       "      <td>-0.838907</td>\n",
       "      <td>0.806007</td>\n",
       "      <td>-0.627346</td>\n",
       "      <td>-0.486310</td>\n",
       "      <td>...</td>\n",
       "      <td>1.274239</td>\n",
       "      <td>-0.773695</td>\n",
       "      <td>-0.644171</td>\n",
       "      <td>1.180180</td>\n",
       "      <td>-0.677068</td>\n",
       "      <td>0.926787</td>\n",
       "      <td>0.647794</td>\n",
       "      <td>-0.538986</td>\n",
       "      <td>-0.337302</td>\n",
       "      <td>0.215343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>-0.276687</td>\n",
       "      <td>-0.391370</td>\n",
       "      <td>0.563168</td>\n",
       "      <td>-0.799415</td>\n",
       "      <td>-0.170027</td>\n",
       "      <td>-0.328408</td>\n",
       "      <td>0.434936</td>\n",
       "      <td>0.044098</td>\n",
       "      <td>-1.251416</td>\n",
       "      <td>-1.841261</td>\n",
       "      <td>...</td>\n",
       "      <td>1.678413</td>\n",
       "      <td>0.708477</td>\n",
       "      <td>-0.357931</td>\n",
       "      <td>0.415079</td>\n",
       "      <td>0.193413</td>\n",
       "      <td>-0.771103</td>\n",
       "      <td>-0.137921</td>\n",
       "      <td>-0.852931</td>\n",
       "      <td>-0.339916</td>\n",
       "      <td>-0.594707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>-0.419423</td>\n",
       "      <td>0.047648</td>\n",
       "      <td>0.352409</td>\n",
       "      <td>-1.285433</td>\n",
       "      <td>-0.459013</td>\n",
       "      <td>-0.435604</td>\n",
       "      <td>-0.815479</td>\n",
       "      <td>0.433315</td>\n",
       "      <td>-1.656348</td>\n",
       "      <td>0.661507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151377</td>\n",
       "      <td>0.902063</td>\n",
       "      <td>-0.022045</td>\n",
       "      <td>0.978881</td>\n",
       "      <td>0.222833</td>\n",
       "      <td>0.035830</td>\n",
       "      <td>1.123409</td>\n",
       "      <td>-0.470635</td>\n",
       "      <td>0.085660</td>\n",
       "      <td>-0.165376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.699847</td>\n",
       "      <td>0.588553</td>\n",
       "      <td>1.259017</td>\n",
       "      <td>-0.445530</td>\n",
       "      <td>1.517912</td>\n",
       "      <td>0.744955</td>\n",
       "      <td>-0.302939</td>\n",
       "      <td>0.081527</td>\n",
       "      <td>-1.575395</td>\n",
       "      <td>-1.017097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034296</td>\n",
       "      <td>-0.604289</td>\n",
       "      <td>-0.996554</td>\n",
       "      <td>1.322647</td>\n",
       "      <td>-0.197088</td>\n",
       "      <td>0.986190</td>\n",
       "      <td>-0.225849</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>-0.696760</td>\n",
       "      <td>-1.105746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>-1.351098</td>\n",
       "      <td>-0.679600</td>\n",
       "      <td>0.525605</td>\n",
       "      <td>0.517367</td>\n",
       "      <td>0.467556</td>\n",
       "      <td>-0.473734</td>\n",
       "      <td>0.739027</td>\n",
       "      <td>0.910557</td>\n",
       "      <td>-0.387079</td>\n",
       "      <td>-1.639311</td>\n",
       "      <td>...</td>\n",
       "      <td>1.256897</td>\n",
       "      <td>-0.127821</td>\n",
       "      <td>0.477409</td>\n",
       "      <td>0.156521</td>\n",
       "      <td>0.266240</td>\n",
       "      <td>-0.228874</td>\n",
       "      <td>-0.794952</td>\n",
       "      <td>-0.817930</td>\n",
       "      <td>-0.345293</td>\n",
       "      <td>0.008012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>-2.174187</td>\n",
       "      <td>-1.395381</td>\n",
       "      <td>-0.367505</td>\n",
       "      <td>-0.444213</td>\n",
       "      <td>-0.158442</td>\n",
       "      <td>-0.585358</td>\n",
       "      <td>-0.976305</td>\n",
       "      <td>0.385204</td>\n",
       "      <td>0.097749</td>\n",
       "      <td>-1.338070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>-0.414148</td>\n",
       "      <td>-0.949915</td>\n",
       "      <td>0.290280</td>\n",
       "      <td>0.283856</td>\n",
       "      <td>-0.273811</td>\n",
       "      <td>0.447147</td>\n",
       "      <td>0.265756</td>\n",
       "      <td>-0.807396</td>\n",
       "      <td>0.670133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "one    -0.835178 -0.119281 -0.440201  0.192750  0.073644 -0.699794  0.977574   \n",
       "people -1.898524  0.805438  0.192769  0.112155  0.050628 -1.142790 -0.281292   \n",
       "like   -0.852569  0.073068 -0.795189 -0.852964  0.465843 -0.275568  0.171036   \n",
       "know   -0.810799  0.276093 -0.233160 -0.071715 -0.062151  0.136346 -0.144619   \n",
       "going  -1.182257  1.100788 -0.105712 -0.579604  0.413323  0.278536 -0.838907   \n",
       "think  -0.276687 -0.391370  0.563168 -0.799415 -0.170027 -0.328408  0.434936   \n",
       "see    -0.419423  0.047648  0.352409 -1.285433 -0.459013 -0.435604 -0.815479   \n",
       "would   0.699847  0.588553  1.259017 -0.445530  1.517912  0.744955 -0.302939   \n",
       "really -1.351098 -0.679600  0.525605  0.517367  0.467556 -0.473734  0.739027   \n",
       "get    -2.174187 -1.395381 -0.367505 -0.444213 -0.158442 -0.585358 -0.976305   \n",
       "\n",
       "              7         8         9   ...        90        91        92  \\\n",
       "one     0.578437 -2.640824 -0.215296  ...  2.062862  0.336131  0.076664   \n",
       "people  1.008322 -1.641191 -2.923650  ...  1.139784  0.489528 -1.488058   \n",
       "like    1.509210 -0.715329  0.387317  ... -0.233583  0.759793  0.243843   \n",
       "know   -0.011745 -0.355997 -1.143154  ...  0.705935 -0.117349 -0.209008   \n",
       "going   0.806007 -0.627346 -0.486310  ...  1.274239 -0.773695 -0.644171   \n",
       "think   0.044098 -1.251416 -1.841261  ...  1.678413  0.708477 -0.357931   \n",
       "see     0.433315 -1.656348  0.661507  ...  0.151377  0.902063 -0.022045   \n",
       "would   0.081527 -1.575395 -1.017097  ...  0.034296 -0.604289 -0.996554   \n",
       "really  0.910557 -0.387079 -1.639311  ...  1.256897 -0.127821  0.477409   \n",
       "get     0.385204  0.097749 -1.338070  ... -0.230000 -0.414148 -0.949915   \n",
       "\n",
       "              93        94        95        96        97        98        99  \n",
       "one     0.108958  0.304847 -1.532940 -0.666886 -0.108505  0.761793  0.945292  \n",
       "people -1.124601 -0.585902  0.197538 -1.239247 -0.041986 -1.946663  1.459767  \n",
       "like   -0.210720  0.659490  0.670538  0.884029  0.539952  0.519874 -0.314470  \n",
       "know   -0.275618 -0.320133  0.630917  0.416345 -0.668252 -0.038271 -0.100392  \n",
       "going   1.180180 -0.677068  0.926787  0.647794 -0.538986 -0.337302  0.215343  \n",
       "think   0.415079  0.193413 -0.771103 -0.137921 -0.852931 -0.339916 -0.594707  \n",
       "see     0.978881  0.222833  0.035830  1.123409 -0.470635  0.085660 -0.165376  \n",
       "would   1.322647 -0.197088  0.986190 -0.225849  0.228122 -0.696760 -1.105746  \n",
       "really  0.156521  0.266240 -0.228874 -0.794952 -0.817930 -0.345293  0.008012  \n",
       "get     0.290280  0.283856 -0.273811  0.447147  0.265756 -0.807396  0.670133  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(model.wv.vectors, index=model.wv.index_to_key).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "968e68c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 임베딩 모델 저장\n",
    "model.wv.save_word2vec_format('ted_en_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52171a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델 로드\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "load_model = KeyedVectors.load_word2vec_format('ted_en_w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a793d9",
   "metadata": {},
   "source": [
    "- 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3433115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8994839191436768),\n",
       " ('daughter', 0.8118333220481873),\n",
       " ('girl', 0.8078429102897644),\n",
       " ('boy', 0.7830139994621277),\n",
       " ('son', 0.781561553478241),\n",
       " ('father', 0.7767151594161987),\n",
       " ('lady', 0.7615271210670471),\n",
       " ('mother', 0.7605016827583313),\n",
       " ('grandfather', 0.7526906728744507),\n",
       " ('sister', 0.7464936971664429)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('man')\n",
    "# model.wv.most_similar('abracadabra')\n",
    "# 임베딩 벡터에 없는 단어로 조회 시 KeyError발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90e14788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8994839191436768),\n",
       " ('daughter', 0.8118333220481873),\n",
       " ('girl', 0.8078429102897644),\n",
       " ('boy', 0.7830139994621277),\n",
       " ('son', 0.781561553478241),\n",
       " ('father', 0.7767151594161987),\n",
       " ('lady', 0.7615271210670471),\n",
       " ('mother', 0.7605016827583313),\n",
       " ('grandfather', 0.7526906728744507),\n",
       " ('sister', 0.7464936971664429)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.most_similar('man')  #Word2Vec.wv = KeyedVectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9f08dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7228153"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man', 'husband')\n",
    "#두 단어간의 유사도를 계산해서 알려줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c31099e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.68057466,  0.07299086,  0.8338339 ,  2.3199794 , -0.7012109 ,\n",
       "       -0.46168062, -0.3453528 ,  1.328402  , -0.28646323, -1.6327552 ,\n",
       "       -0.07507036,  0.84741825, -0.54242015,  0.45442292,  0.6876908 ,\n",
       "       -0.22775438,  0.49946275,  0.32352328, -1.1262149 , -0.8850733 ,\n",
       "        1.0560582 ,  0.48169017,  0.54094917,  0.5267468 ,  0.2508988 ,\n",
       "        0.4044799 , -1.5711879 , -0.26845998,  0.19729894,  0.9443723 ,\n",
       "       -1.0728304 , -1.0971597 ,  0.19572815, -1.3189031 , -0.3877103 ,\n",
       "        0.61173517, -0.16726735, -0.75763243,  0.9267616 ,  0.08264314,\n",
       "        0.5694112 ,  0.02602324,  0.5099423 ,  0.7711608 ,  1.5683501 ,\n",
       "        0.07310363, -0.05430914,  0.6580541 ,  0.90930045,  0.33769444,\n",
       "        0.28642762, -0.38574627, -0.41201454, -0.71984214,  1.0887781 ,\n",
       "        0.43206123, -0.2541912 ,  0.58020407, -0.38206455, -0.04806631,\n",
       "        0.08054022, -0.16801128, -0.9421462 ,  1.0940529 , -0.85190177,\n",
       "        0.76574105, -0.61811066,  0.6296726 ,  0.8995828 ,  1.6911675 ,\n",
       "       -1.2065336 , -1.2676916 ,  0.2242219 , -1.300466  ,  0.20709707,\n",
       "        0.6561978 ,  0.22821249,  0.14041229,  0.20559503, -0.16188626,\n",
       "       -0.8740652 ,  0.5093269 , -1.0174832 ,  1.0429808 , -0.4410745 ,\n",
       "       -0.04376404, -0.520124  , -0.44727832,  0.5192348 , -0.0608527 ,\n",
       "       -0.1536811 , -0.37258607,  0.6664514 , -1.6754551 ,  0.25578877,\n",
       "        0.03891801,  0.8068729 ,  0.23879766, -0.47967902, -1.266787  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['man']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878567c",
   "metadata": {},
   "source": [
    "- 임베딩 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a135644",
   "metadata": {},
   "source": [
    "https://projector.tensorflow.org/\n",
    "\n",
    "- embedding vector(tensor) 파일 (.tsv)\n",
    "- metadat 파일 (.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b7e9019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 09:49:08,349 - word2vec2tensor - INFO - running c:\\Users\\maest\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\gensim\\scripts\\word2vec2tensor.py --input ted_en_w2v --output ted_en_w2v\n",
      "2025-04-08 09:49:08,349 - keyedvectors - INFO - loading projection weights from ted_en_w2v\n",
      "2025-04-08 09:49:09,598 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (21462, 100) matrix of type float32 from ted_en_w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-04-08T09:49:09.518506', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:16) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22631-SP0', 'event': 'load_word2vec_format'}\n",
      "2025-04-08 09:49:10,395 - word2vec2tensor - INFO - 2D tensor file saved to ted_en_w2v_tensor.tsv\n",
      "2025-04-08 09:49:10,395 - word2vec2tensor - INFO - Tensor metadata file saved to ted_en_w2v_metadata.tsv\n",
      "2025-04-08 09:49:10,396 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m  gensim.scripts.word2vec2tensor --input ted_en_w2v --output ted_en_w2v  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9512a7",
   "metadata": {},
   "source": [
    "### 한국어 Word Embedding\n",
    "- NSMC (Naver Sentiment Movie Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca8c0dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4686d571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('naver_movie_ratings.txt', <http.client.HTTPMessage at 0x240ac0a0500>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 다운로드\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", \n",
    "    filename = \"naver_movie_ratings.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05bb8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 프레임 생성\n",
    "ratings_df = pd.read_csv('naver_movie_ratings.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d84b2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    8\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#결측치 확인 및 처리\n",
    "display(ratings_df.isnull().sum())\n",
    "\n",
    "ratings_df = ratings_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4cc7fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200    많은 생각을 할 수 있는 영화~ 시간여행류의 스토리를 좋아하는 사람이라면 빠트릴 수...\n",
       "201    고소한 19 정말 재미있게 잘 보고 있습니다^^ 방송만 보면 털털하고 인간적이신 것...\n",
       "202                                                  가연세\n",
       "203                         goodgoodgoodgoodgoodgoodgood\n",
       "204                                           이물감. 시 같았다\n",
       "                             ...                        \n",
       "295                                   박력넘치는 스턴트 액션 평작이다!\n",
       "296                                      엄청 재미있다 명작이다 ~~\n",
       "297    나는 하정우랑 개그코드가 맞나보다 엄청 재밌게봤네요 특히 단발의사샘 장면에서 계속 ...\n",
       "298                                                적당 ㅎㅎ\n",
       "299                                    배경이 이쁘고 캐릭터도 귀엽네~\n",
       "Name: document, Length: 100, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df['document'][200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30ffe725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#한글이 아닌 데이터 제거 (숫자와 한글 데이터 전부 포함)\n",
    "ratings_df['document'] = ratings_df['document'].replace(r'[^0-9가-힣ㄱ-ㅎㅏ-ㅣ\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e0245f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199992/199992 [06:41<00:00, 498.66it/s]\n"
     ]
    }
   ],
   "source": [
    "#전처리\n",
    "from tqdm import tqdm   #진행도 시각화\n",
    "\n",
    "okt = Okt()\n",
    "ko_stopwords = ['은', '는', '이', '가', '을', '를', '와', '과', '들', '도', '부터', '까지', '에', '나', '너', '그', '걔', '얘']\n",
    "\n",
    "preprocessed_data = []\n",
    "\n",
    "for sentence in tqdm(ratings_df['document']):\n",
    "    tokens = okt.morphs(sentence, stem=True)\n",
    "    tokens = [token for token in tokens if token not in ko_stopwords]\n",
    "    preprocessed_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3fc8248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16841, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(\n",
    "    sentences = preprocessed_data,\n",
    "    vector_size = 100,\n",
    "    window = 5,\n",
    "    min_count = 5,\n",
    "    sg=0        #CBOW\n",
    ")\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a389abe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('영화관', 0.9374942183494568),\n",
       " ('틀어주다', 0.7930522561073303),\n",
       " ('케이블', 0.7774772644042969),\n",
       " ('학교', 0.7411223649978638),\n",
       " ('티비', 0.7175014019012451),\n",
       " ('메가박스', 0.7062078714370728),\n",
       " ('개봉관', 0.6908875107765198),\n",
       " ('영화제', 0.687961757183075),\n",
       " ('명화극장', 0.6869919896125793),\n",
       " ('방금', 0.6835576891899109)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('극장')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d4d74fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.793258"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('김혜수', '전지현')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4d8b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장\n",
    "model.wv.save_word2vec_format('naver_movie_ratings_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f1d6df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 09:56:01,716 - word2vec2tensor - INFO - running c:\\Users\\maest\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\gensim\\scripts\\word2vec2tensor.py --input naver_movie_ratings_w2v --output naver_movie_ratings_w2v\n",
      "2025-04-08 09:56:01,716 - keyedvectors - INFO - loading projection weights from naver_movie_ratings_w2v\n",
      "2025-04-08 09:56:02,680 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (16841, 100) matrix of type float32 from naver_movie_ratings_w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-04-08T09:56:02.579759', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:16) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22631-SP0', 'event': 'load_word2vec_format'}\n",
      "2025-04-08 09:56:03,301 - word2vec2tensor - INFO - 2D tensor file saved to naver_movie_ratings_w2v_tensor.tsv\n",
      "2025-04-08 09:56:03,301 - word2vec2tensor - INFO - Tensor metadata file saved to naver_movie_ratings_w2v_metadata.tsv\n",
      "2025-04-08 09:56:03,302 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m  gensim.scripts.word2vec2tensor --input naver_movie_ratings_w2v --output naver_movie_ratings_w2v "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb793c",
   "metadata": {},
   "source": [
    "- 사전 훈련된 임베딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24834f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1aL_xpWW-CjfCrLWeflIaipITOZ6zHI5c\n",
      "From (redirected): https://drive.google.com/uc?id=1aL_xpWW-CjfCrLWeflIaipITOZ6zHI5c&confirm=t&uuid=2d350792-36b3-49d5-a9fe-986b651e845d\n",
      "To: c:\\encore-skn11\\07_NLP\\03_word_embedding\\GoogleNews_vecs.bins.gz\n",
      "100%|██████████| 1.65G/1.65G [00:37<00:00, 44.2MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GoogleNews_vecs.bins.gz'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://drive.google.com/uc?id=1aL_xpWW-CjfCrLWeflIaipITOZ6zHI5c\"\n",
    "output = \"GoogleNews_vecs.bins.gz\"\n",
    "\n",
    "gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53b4446a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv = KeyedVectors.load_word2vec_format('GoogleNews_vecs.bins.gz', binary=True)\n",
    "google_news_wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a3911c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22942671"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.similarity('king','man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34d470f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138045430183411),\n",
       " ('queen', 0.6510957479476929),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204220056533813),\n",
       " ('prince', 0.6159993410110474),\n",
       " ('sultan', 0.5864824056625366),\n",
       " ('ruler', 0.5797566771507263),\n",
       " ('princes', 0.5646551847457886),\n",
       " ('Prince_Paras', 0.5432944297790527),\n",
       " ('throne', 0.5422105193138123)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d97a5868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24791393"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.n_similarity(['king', 'queen'], ['man', 'woman'])\n",
    "#리스트 간의 평균 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c27b4e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22942671"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.similarity('king','man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da3b526d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138045430183411),\n",
       " ('queen', 0.6510957479476929),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204220056533813),\n",
       " ('prince', 0.6159993410110474)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.most_similar('king', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e80ed7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24791393"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.n_similarity(['king', 'queen'], ['man', 'woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7207192c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138045430183411),\n",
       " ('queen', 0.6510957479476929),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204220056533813),\n",
       " ('prince', 0.6159993410110474),\n",
       " ('sultan', 0.5864824056625366),\n",
       " ('ruler', 0.5797566771507263),\n",
       " ('princes', 0.5646551847457886),\n",
       " ('Prince_Paras', 0.5432944297790527),\n",
       " ('throne', 0.5422105193138123)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.similar_by_word('king')  #most_similar 과 같은 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c36522b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.has_index_for('ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ')    \n",
    "#()안에 있는 텍스트가 있는지 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
